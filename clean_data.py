"""
æ•°æ®å»é‡å’Œæ¸…ç†å·¥å…·
"""
import csv
import pandas as pd
from datetime import datetime

def remove_duplicates_from_csv(input_file, output_file=None):
    """ä»CSVæ–‡ä»¶ä¸­å»é™¤é‡å¤æ•°æ®"""
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(input_file, encoding='utf-8-sig')
        
        print(f"åŸå§‹æ•°æ®: {len(df)} è¡Œ")
        print(f"å­—æ®µ: {list(df.columns)}")
        
        # åŸºäºBVå·å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•
        df_unique = df.drop_duplicates(subset=['BVå·'], keep='first')
        
        print(f"å»é‡å: {len(df_unique)} è¡Œ")
        print(f"ç§»é™¤é‡å¤: {len(df) - len(df_unique)} è¡Œ")
        
        # é‡æ–°ç¼–å·
        df_unique = df_unique.reset_index(drop=True)
        df_unique['åºå·'] = range(1, len(df_unique) + 1)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"bilibili_videos_cleaned_{timestamp}.csv"
        
        # ä¿å­˜å»é‡åçš„æ•°æ®
        df_unique.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®
        print("\nğŸ“º å‰10ä¸ªè§†é¢‘:")
        print("-" * 80)
        for i, row in df_unique.head(10).iterrows():
            title = row['è§†é¢‘æ ‡é¢˜'][:50] + "..." if len(str(row['è§†é¢‘æ ‡é¢˜'])) > 50 else row['è§†é¢‘æ ‡é¢˜']
            print(f"{row['åºå·']:2d}. {title:55s} | {row['BVå·']}")
        
        if len(df_unique) > 10:
            print(f"    ... è¿˜æœ‰ {len(df_unique) - 10} ä¸ªè§†é¢‘")
        
        return output_file
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return None

def analyze_csv_data(filename):
    """åˆ†æCSVæ•°æ®"""
    try:
        df = pd.read_csv(filename, encoding='utf-8-sig')
        
        print("=" * 60)
        print(f"ğŸ“Š æ•°æ®åˆ†æ: {filename}")
        print("=" * 60)
        print(f"æ€»è®°å½•æ•°: {len(df)}")
        print(f"å­—æ®µæ•°: {len(df.columns)}")
        print(f"å­—æ®µå: {list(df.columns)}")
        
        if 'BVå·' in df.columns:
            unique_bv = df['BVå·'].nunique()
            total_bv = len(df)
            print(f"å”¯ä¸€BVå·: {unique_bv}")
            print(f"é‡å¤è®°å½•: {total_bv - unique_bv}")
        
        if 'é¡µç ' in df.columns:
            page_counts = df['é¡µç '].value_counts().sort_index()
            print(f"é¡µé¢åˆ†å¸ƒ: {dict(page_counts)}")
        
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

def main():
    print("ğŸ§¹ Bç«™è§†é¢‘æ•°æ®æ¸…ç†å·¥å…·")
    print("=" * 60)
    
    # æŸ¥æ‰¾æ‰€æœ‰bilibiliè§†é¢‘CSVæ–‡ä»¶
    import glob
    csv_files = glob.glob("bilibili_videos*.csv")
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°bilibiliè§†é¢‘CSVæ–‡ä»¶")
        return
    
    print("ğŸ“ æ‰¾åˆ°ä»¥ä¸‹CSVæ–‡ä»¶:")
    for i, file in enumerate(csv_files, 1):
        print(f"{i}. {file}")
    
    # å¤„ç†æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(csv_files, key=lambda x: x.split('_')[-1] if '_' in x else x)
    print(f"\nğŸ¯ å¤„ç†æœ€æ–°æ–‡ä»¶: {latest_file}")
    
    # åˆ†æåŸå§‹æ•°æ®
    analyze_csv_data(latest_file)
    
    # å»é‡å¤„ç†
    print("\nğŸ§¹ å¼€å§‹å»é‡å¤„ç†...")
    cleaned_file = remove_duplicates_from_csv(latest_file)
    
    if cleaned_file:
        print(f"\nâœ… å¤„ç†å®Œæˆï¼")
        print(f"åŸæ–‡ä»¶: {latest_file}")
        print(f"æ¸…ç†å: {cleaned_file}")
    
    # æä¾›æ‰€æœ‰æ–‡ä»¶çš„å¿«é€Ÿåˆ†æ
    print("\nğŸ“ˆ æ‰€æœ‰æ–‡ä»¶å¿«é€Ÿç»Ÿè®¡:")
    print("-" * 60)
    for file in csv_files:
        try:
            df = pd.read_csv(file, encoding='utf-8-sig')
            unique_count = df['BVå·'].nunique() if 'BVå·' in df.columns else 0
            total_count = len(df)
            print(f"{file:35s} | æ€»è®¡:{total_count:3d} | å”¯ä¸€:{unique_count:3d} | é‡å¤:{total_count-unique_count:3d}")
        except:
            print(f"{file:35s} | è¯»å–å¤±è´¥")

if __name__ == "__main__":
    main()
