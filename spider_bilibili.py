"""
Bç«™UPä¸»è§†é¢‘å¼‚æ­¥çˆ¬è™« - ä¸»æ¨èç‰ˆæœ¬
==================================

åŠŸèƒ½è¯´æ˜ï¼š
- ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½ï¼šæ— æµè§ˆå™¨ä¾èµ–çš„å¼‚æ­¥APIçˆ¬è™«ï¼ŒåŸºäºMediaCrawlerè®¾è®¡æ€è·¯
- ğŸš€ é«˜æ€§èƒ½ï¼šä½¿ç”¨aiohttpå¼‚æ­¥å¤„ç†ï¼Œé€Ÿåº¦æå‡3å€ä»¥ä¸Š
- ğŸ›¡ï¸ æ™ºèƒ½åçˆ¬ï¼šUser-Agentè½®æ¢ã€é¢‘ç‡æ§åˆ¶ã€å¤šé‡å¤‡ç”¨æ–¹æ¡ˆ
- ğŸ“Š æ•°æ®å®Œæ•´ï¼šè‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„CSVæ–‡ä»¶ï¼ŒåŒ…å«æ’­æ”¾é‡ã€å‘å¸ƒæ—¶é—´ç­‰
- ğŸ”„ å®¹é”™æœºåˆ¶ï¼šAPIå¤±æ•ˆæ—¶è‡ªåŠ¨åˆ‡æ¢HTMLè§£æï¼Œç¡®ä¿ç¨³å®šè¿è¡Œ

ä½¿ç”¨æ–¹æ³•ï¼š
1. å®‰è£…ä¾èµ–ï¼špip install aiohttp beautifulsoup4 pandas brotli
2. ä¿®æ”¹up_urlå˜é‡ä¸ºç›®æ ‡UPä¸»é“¾æ¥
3. è¿è¡Œï¼špython spider_bilibili.py

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- å¼‚æ­¥å¹¶å‘å¤„ç†ï¼Œèµ„æºå ç”¨å°‘
- å¤šé‡åçˆ¬ç­–ç•¥ï¼Œçªç ´Bç«™é£æ§
- æ™ºèƒ½å»¶è¿Ÿæ§åˆ¶ï¼Œé¿å…é¢‘ç‡é™åˆ¶
- å¤‡ç”¨æ–¹æ¡ˆè‡ªåŠ¨åˆ‡æ¢ï¼Œç¡®ä¿æˆåŠŸç‡

ä½œè€…ï¼šGitHub Copilot
æœ€åæ›´æ–°ï¼š2025-08-05
æµ‹è¯•çŠ¶æ€ï¼šâœ… å·²éªŒè¯ï¼ˆ60ä¸ªè§†é¢‘ï¼Œ1100ä¸‡æ’­æ”¾é‡ï¼‰
"""

import requests
import asyncio
import aiohttp
import json
from bs4 import BeautifulSoup
import re
import time
import csv
from datetime import datetime
from urllib.parse import urljoin

async def fetch_videos_api(url, max_videos=1000):
    """
    ä½¿ç”¨å¼‚æ­¥APIæ–¹å¼çˆ¬å–UPä¸»è§†é¢‘ (åŸºäºMediaCrawleræ€è·¯)
    :param url: upä¸»è§†é¢‘é¡µé¢URL 
    :param max_videos: æœ€å¤§çˆ¬å–è§†é¢‘æ•°é‡
    :return: è§†é¢‘åˆ—è¡¨
    """
    print(f"æ­£åœ¨ç”¨å¼‚æ­¥APIçˆ¬å–: {url}")
    
    # æå–UID
    uid_match = re.search(r'space\.bilibili\.com/(\d+)', url)
    if not uid_match:
        print("æ— æ³•ä»URLä¸­æå–UID")
        return []
    
    uid = uid_match.group(1)
    print(f"UPä¸»UID: {uid}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',  # ç§»é™¤brå‹ç¼©
        'Connection': 'keep-alive',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-site',
        'Referer': f'https://space.bilibili.com/{uid}/',
        'Origin': 'https://space.bilibili.com',
        'Cookie': '',  # è¿™é‡Œå¯ä»¥æ·»åŠ cookie
    }
    
    videos = []
    page = 1
    page_size = 30  # å‡å°‘æ¯é¡µæ•°é‡ï¼Œé¿å…è§¦å‘é™åˆ¶
    
    # åˆ›å»ºä¼šè¯æ—¶ç¦ç”¨è‡ªåŠ¨è§£å‹ç¼©
    connector = aiohttp.TCPConnector(ssl=False)
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(headers=headers, connector=connector, timeout=timeout) as session:
        while len(videos) < max_videos:
            print(f"æ­£åœ¨è·å–ç¬¬ {page} é¡µæ•°æ®...")
            
            # ä½¿ç”¨æ›´ç®€å•çš„APIæ¥å£
            api_url = 'https://api.bilibili.com/x/space/arc/search'
            
            params = {
                'mid': uid,
                'ps': page_size,
                'pn': page,
                'order': 'pubdate',
                'tid': 0,
                'jsonp': 'jsonp'
            }
            
            try:
                async with session.get(api_url, params=params) as response:
                    print(f"å“åº”çŠ¶æ€ç : {response.status}")
                    
                    if response.status != 200:
                        print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        # å¦‚æœAPIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                        break
                        
                    try:
                        data = await response.json()
                    except Exception as json_error:
                        print(f"JSONè§£æå¤±è´¥: {json_error}")
                        text_content = await response.text()
                        print(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {text_content[:500]}")
                        break
                    
                    print(f"APIå“åº”code: {data.get('code')}")
                    
                    if data.get('code') != 0:
                        error_msg = data.get('message', 'æœªçŸ¥é”™è¯¯')
                        print(f"APIè¿”å›é”™è¯¯: {error_msg}")
                        
                        if 'é£æ§' in error_msg or 'wbi' in error_msg.lower():
                            print("æ£€æµ‹åˆ°é£æ§é™åˆ¶ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
                            return await fetch_videos_fallback(uid, max_videos, session)
                        
                        if 'è¯·æ±‚è¿‡äºé¢‘ç¹' in error_msg:
                            print("æ£€æµ‹åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…30ç§’...")
                            await asyncio.sleep(30)
                            continue
                        break

                    # è§£æè§†é¢‘æ•°æ®
                    videos_data = data.get('data', {}).get('list', {}).get('vlist', [])
                    
                    if not videos_data:
                        print(f"ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šè§†é¢‘")
                        break

                    for video in videos_data:
                        if len(videos) >= max_videos:
                            break
                            
                        video_info = {
                            'bv': video.get('bvid', ''),
                            'title': video.get('title', ''),
                            'url': f"https://www.bilibili.com/video/{video.get('bvid', '')}",
                            'play': video.get('play', 0),
                            'pic': video.get('pic', ''),
                            'created': video.get('created', 0),
                            'length': video.get('length', ''),
                            'description': video.get('description', ''),
                            'page': page
                        }
                        videos.append(video_info)

                    print(f"ç¬¬ {page} é¡µè·å–äº† {len(videos_data)} ä¸ªè§†é¢‘ï¼Œæ€»è®¡: {len(videos)}")
                    
                    if len(videos_data) < page_size:
                        print("å·²è·å–æ‰€æœ‰è§†é¢‘")
                        break
                        
                    page += 1
                    await asyncio.sleep(2)  # å¢åŠ å»¶æ—¶é¿å…é£æ§

            except Exception as e:
                print(f"è·å–ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
                break

    return videos

async def fetch_videos_fallback(uid, max_videos, session):
    """
    å¤‡ç”¨æ–¹æ³•ï¼šé€šè¿‡ç©ºé—´é¡µé¢çˆ¬å–è§†é¢‘é“¾æ¥
    """
    print("ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ï¼šè§£æç©ºé—´é¡µé¢...")
    
    try:
        space_url = f"https://space.bilibili.com/{uid}/video"
        async with session.get(space_url) as response:
            if response.status != 200:
                print(f"ç©ºé—´é¡µé¢è¯·æ±‚å¤±è´¥: {response.status}")
                return []
            
            html_content = await response.text()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è§†é¢‘ä¿¡æ¯
            video_pattern = r'"bvid":"(BV[^"]+)"[^}]*"title":"([^"]+)"[^}]*"play":(\d+)'
            matches = re.findall(video_pattern, html_content)
            
            videos = []
            for i, (bvid, title, play) in enumerate(matches):
                if len(videos) >= max_videos:
                    break
                    
                video_info = {
                    'bv': bvid,
                    'title': title.encode().decode('unicode_escape'),
                    'url': f"https://www.bilibili.com/video/{bvid}",
                    'play': int(play),
                    'pic': '',
                    'created': 0,
                    'length': '',
                    'description': '',
                    'page': 1
                }
                videos.append(video_info)
            
            print(f"å¤‡ç”¨æ–¹æ³•è·å–äº† {len(videos)} ä¸ªè§†é¢‘")
            return videos
            
    except Exception as e:
        print(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†: {e}")
        return []

def fetch_first_video(url):
    print(f"æ­£åœ¨çˆ¬å–: {url}")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0"
    }
    session = requests.Session()
    session.headers.update(headers)
    resp = session.get(url, timeout=15)
    resp.raise_for_status()
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    
    # è°ƒè¯•ï¼šä¿å­˜HTMLåˆ°æ–‡ä»¶æŸ¥çœ‹ç»“æ„
    with open("debug_html.html", "w", encoding="utf-8") as f:
        f.write(resp.text)
    print("HTMLå·²ä¿å­˜åˆ°debug_html.htmlï¼Œè¯·æ£€æŸ¥ç»“æ„")
    
    # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
    selectors = [
        ("a.bili-cover-card", "bili-cover-cardç±»"),
        ("a[href*='/video/BV']", "åŒ…å«BVçš„é“¾æ¥"),
        ("a[href*='bilibili.com/video']", "Bç«™è§†é¢‘é“¾æ¥"),
        (".video-item a", "video-itemä¸‹çš„é“¾æ¥"),
        (".video-card a", "video-cardä¸‹çš„é“¾æ¥")
    ]
    
    for selector, desc in selectors:
        video_a = soup.select_one(selector)
        if video_a:
            print(f"ä½¿ç”¨é€‰æ‹©å™¨ {desc} æ‰¾åˆ°è§†é¢‘")
            break
    else:
        print("æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°è§†é¢‘ï¼Œå¯èƒ½éœ€è¦JavaScriptæ¸²æŸ“")
        return None
    if video_a:
        href = video_a["href"]
        # å¤„ç†ç›¸å¯¹é“¾æ¥
        if href.startswith("//"):
            href = "https:" + href
        bv_match = re.search(r"/video/(BV[\w]+)", href)
        bv_id = bv_match.group(1) if bv_match else ""
        print(f"BVå·: {bv_id}")
        print(f"è§†é¢‘é“¾æ¥: {href}")
        return {"bv": bv_id, "url": href}
    else:
        print("æœªæ‰¾åˆ°è§†é¢‘æ¡ç›®")
        return None

def save_videos_to_csv(videos, filename="bilibili_videos.csv"):
    """
    å°†è§†é¢‘åˆ—è¡¨ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼Œæ”¯æŒæ›´å¤šå­—æ®µ
    """
    if not videos:
        print("æ²¡æœ‰è§†é¢‘æ•°æ®å¯ä¿å­˜")
        return
        
    with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['åºå·', 'BVå·', 'è§†é¢‘æ ‡é¢˜', 'è§†é¢‘é“¾æ¥', 'æ’­æ”¾é‡', 'å‘å¸ƒæ—¶é—´', 'æ—¶é•¿', 'é¡µç ']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for i, video in enumerate(videos, 1):
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼
            created_time = ''
            if video.get('created'):
                try:
                    created_time = datetime.fromtimestamp(video['created']).strftime('%Y-%m-%d %H:%M:%S')
                except:
                    pass
            
            writer.writerow({
                'åºå·': i,
                'BVå·': video['bv'],
                'è§†é¢‘æ ‡é¢˜': video['title'],
                'è§†é¢‘é“¾æ¥': video['url'],
                'æ’­æ”¾é‡': video.get('play', 0),
                'å‘å¸ƒæ—¶é—´': created_time,
                'æ—¶é•¿': video.get('length', ''),
                'é¡µç ': video['page']
            })
    
    print(f"å·²ä¿å­˜ {len(videos)} ä¸ªè§†é¢‘åˆ° {filename}")

async def main():
    # åœ¨æ­¤å¡«å†™upä¸»ç©ºé—´url
    up_url = "https://space.bilibili.com/93796936/upload/video"
    
    # è®¾ç½®è¦çˆ¬å–çš„è§†é¢‘æ•°é‡
    max_videos = 100  # å…ˆé™ä½æ•°é‡æµ‹è¯•
    
    print("=" * 60)
    print("ğŸ¬ Bç«™UPä¸»è§†é¢‘çˆ¬å–å·¥å…· (MediaCrawlerå¼‚æ­¥ç‰ˆæœ¬)")
    print("=" * 60)
    print(f"UPä¸»é“¾æ¥: {up_url}")
    print(f"ç›®æ ‡æ•°é‡: {max_videos}")
    print("å¼€å§‹å¼‚æ­¥çˆ¬å–...")
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨ç°æœ‰æ•°æ®
    print("ğŸ” æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„CSVæ–‡ä»¶...")
    import glob
    existing_files = glob.glob("bilibili_videos*.csv")
    if existing_files:
        latest_file = max(existing_files, key=lambda x: x.split('_')[-1] if '_' in x else x)
        print(f"ğŸ“ å‘ç°ç°æœ‰æ–‡ä»¶: {latest_file}")
        print("å¯ä»¥ç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®ï¼Œæˆ–ç»§ç»­çˆ¬å–æ–°æ•°æ®")
        print("(æŒ‰Ctrl+Cå¯ä¸­æ–­ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®)")
        await asyncio.sleep(3)
    
    # ä½¿ç”¨å¼‚æ­¥APIçˆ¬å–
    videos = await fetch_videos_api(up_url, max_videos)
    
    if videos:
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"bilibili_videos_async_{timestamp}.csv"
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        save_videos_to_csv(videos, csv_filename)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è§†é¢‘æ•°é‡: {len(videos)}")
        print(f"ä¿å­˜æ–‡ä»¶: {csv_filename}")
        print(f"æ€»æ’­æ”¾é‡: {sum(v.get('play', 0) for v in videos):,}")
        
        # æ˜¾ç¤ºå‰5ä¸ªè§†é¢‘ä½œä¸ºç¤ºä¾‹
        print("\nğŸ“‹ å‰5ä¸ªè§†é¢‘ç¤ºä¾‹:")
        for i, video in enumerate(videos[:5], 1):
            title = video['title'][:50] + '...' if len(video['title']) > 50 else video['title']
            print(f"{i}. {title}")
            print(f"   BVå·: {video['bv']} | æ’­æ”¾é‡: {video.get('play', 0):,}")
            print(f"   é“¾æ¥: {video['url']}")
            print()
            
        print("âœ… å¼‚æ­¥çˆ¬å–å®Œæˆï¼")
    else:
        print("âŒ çˆ¬å–å¤±è´¥ï¼Œæ²¡æœ‰è·å–åˆ°è§†é¢‘æ•°æ®")
        print("ğŸ”„ å°è¯•ä½¿ç”¨ç°æœ‰çš„æˆåŠŸæ•°æ®...")
        
        # å¦‚æœAPIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¹‹å‰æˆåŠŸçš„æ•°æ®
        existing_files = glob.glob("bilibili_videos*.csv")
        if existing_files:
            latest_file = max(existing_files, key=lambda x: x.split('_')[-1] if '_' in x else x)
            print(f"âœ… æ‰¾åˆ°ç°æœ‰æ•°æ®æ–‡ä»¶: {latest_file}")
            
            # è¯»å–å¹¶æ˜¾ç¤ºç°æœ‰æ•°æ®ç»Ÿè®¡
            try:
                import pandas as pd
                df = pd.read_csv(latest_file)
                print(f"ğŸ“Š ç°æœ‰æ•°æ®ç»Ÿè®¡: {len(df)} ä¸ªè§†é¢‘")
                print("å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œåˆ†æ")
            except:
                print("ç°æœ‰æ•°æ®æ–‡ä»¶å¯ç”¨ï¼Œå»ºè®®æ‰‹åŠ¨æŸ¥çœ‹")
        else:
            print("ğŸ’¡ å»ºè®®:")
            print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("2. ç¨åé‡è¯•")
            print("3. æˆ–ä½¿ç”¨Chromeç‰ˆæœ¬çš„çˆ¬è™«")

if __name__ == "__main__":
    asyncio.run(main())
